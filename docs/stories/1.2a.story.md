# Story 1.2a - Azure OpenAI STT End-to-End Integration

**Story ID:** 1.2a  
**Epic:** Epic 1: Foundation & Core Voice Infrastructure  
**Status:** Ready for Review  
**Story Points:** 5 (Implementation + wiring)  
**Priority:** P0 (blocks meaningful voice input usage)

---

## Story

As a Vietnamese numerology seeker,
I want my recorded voice messages to be transcribed through the new Azure OpenAI gpt-4o-mini-transcribe service,
so that the mobile app reflects the real cost and accuracy improvements promised in Story 1.2.

---

## Problem Statement & Context

Story 1.2 swapped the speech provider on paper but stopped short of wiring the new SDK into a callable API.  The `AzureOpenAISpeechToTextService` exists in the backend, yet no FastAPI route invokes it, the mobile client still streams to a non-existent `/ws/voice` WebSocket, and project documentation continues to direct developers to Azure Speech Services.  This story delivers the glue so the mobile flow actually uses Azure OpenAI gpt-4o-mini-transcribe, proving the cost/accuracy benefits and unblocking downstream conversation stories.

---

## Acceptance Criteria

1. **Backend API Endpoint Available**  
   - A FastAPI endpoint (e.g. `POST /api/v1/voice/transcriptions`) accepts a single WAV/PCM16 audio payload (multipart/form-data).  
   - Endpoint uses `AzureOpenAISpeechToTextService.transcribe_audio_stream` to obtain the transcript.  
   - Response JSON matches mobile expectations: `{ text, confidence, alternatives[], durationMs }`.  
   - Invalid or missing audio returns structured Vietnamese errors (HTTP 400/415).  
   - Endpoint covered by unit/integration tests with fixture audio.

2. **Mobile Client Sends Recorded Audio to New Endpoint**  
   - `speech-to-text.ts` (and any related orchestration) posts the recorded WAV from Expo Audio to the new REST endpoint.  
   - Removes obsolete `/ws/voice` WebSocket logic and TODO placeholders.  
   - Handles progress, errors, and state updates via Zustand store; preserves waveform + haptics.  
   - Works on iOS, Android, and web (Expo) when the backend runs locally.

3. **Config & Docs Reflect Azure OpenAI Flow**  
   - `.env.example`, `SETUP_GUIDE.md`, `START_SERVER.md`, and `apps/api/README.md` describe Azure OpenAI STT instead of Azure Speech Services.  
   - `docs/azure-setup-guide.md` either (a) gains an Azure OpenAI section or (b) clearly states it is legacy and links to the new guide.  
   - README quick-start steps mention the new endpoint for manual verification.

4. **Logging, Metrics & Errors Traceable**  
   - Backend logs include deployment name, request ID, duration, and confidence.  
   - Errors from Azure OpenAI map to Vietnamese messages already defined in `voice_service`.  
   - Mobile surfaces errors to the user and records them in the debug log (Zustand).

5. **Tests Green & Regression Coverage Added**  
   - New backend tests for the endpoint (happy path + error path).  
   - Mobile Jest tests updated to cover REST client logic.  
   - Existing voice-related suites pass (`nx test mobile`, `pytest src/__tests__/`).

---

## Tasks & Subtasks

- [x] **Task 1: Design the REST Interface**  
  - [x] Define request/response schema in a Pydantic model (duration, confidence, alternatives).  
  - [x] Document endpoint contract in OpenAPI (FastAPI router) and README.

- [x] **Task 2: Implement Backend Endpoint**  
  - [x] Create `apps/api/src/routes/voice.py` (or similar) and register with FastAPI.  
  - [x] Convert uploaded audio file/stream into the bytes generator expected by `voice_service`.  
  - [x] Log telemetry (duration, user ID header optional).  
  - [x] Add pytest covering success, missing file, unsupported content-type.

- [x] **Task 3: Update Mobile Speech-to-Text Client**  
  - [x] Replace WebSocket logic with REST upload using `fetch`/`axios`.  
  - [x] Ensure recorded file is converted/form-data friendly (Expo FileSystem).  
  - [x] Maintain waveform + haptics using local state while awaiting response.  
  - [x] Update/unit-test service and store interactions.

- [x] **Task 4: Refine Documentation & Env Templates**  
  - [x] Update READMEs, setup guides, and `.env.example` to highlight Azure OpenAI STT.  
  - [x] Add quick verification command (curl) hitting the new endpoint.  
  - [x] Note deprecation of Azure Speech guide or refactor it into Azure OpenAI instructions.

- [x] **Task 5: QA & Validation**  
  - [ ] Run local end-to-end test: record sample voice in Expo, verify transcript returned. *(curl + automated tests only; Expo manual run still pending)*  
  - [x] Confirm cost/accuracy logging produced in backend logs.  
  - [x] Capture manual test notes in QA section of story file.

---

## Dependencies & References

- Story 1.2 (Voice Input & Speech Recognition Integration) – foundational UI/state work.  
- Story 1.6 (Voice Services refactor) – provides the AzureOpenAISpeechToTextService class.  
- `apps/api/src/services/voice_service.py` – SDK-based transcription implementation.  
- Expo Audio recording flow established in `apps/mobile/src/services/audio.ts`.

External resources:  
- Azure OpenAI Audio Transcriptions API docs (2025-01-01-preview).  
- Expo FileSystem & Network docs for uploading binary data.  
- FastAPI `UploadFile` docs for streaming file uploads.

---

## Testing Strategy

**Backend**  
- Pytest unit tests for new route (mocking Azure client).  
- Integration test using a short Vietnamese WAV fixture to validate 200/400/415 paths.  
- Logging assertions via captured logs.

**Mobile**  
- Jest tests mocking `fetch/axios` to ensure correct form-data payload.  
- Zustand store tests verifying state transitions on success/failure.  
- Manual device testing (iOS/Android) to confirm microphone permissions + response latency (<2s).  

**Manual Verification**  
1. Run backend with Azure credentials configured.  
2. Use provided CLI script or curl command in README to upload `sample_vi.wav`.  
3. Launch Expo dev server, press voice button, verify transcript text and editing UI.  
4. Confirm Vietnamese error text when disconnecting network mid-upload.

---

## File List (expected changes)

- `docs/stories/1.2a.story.md` (this document)  
- `apps/api/src/routes/voice.py`  
- `apps/api/src/schemas/voice.py`  
- `apps/api/src/__tests__/test_voice_endpoint.py`  
- `apps/api/src/main.py`  
- `apps/api/src/middleware/error_handler.py`  
- `apps/api/src/services/voice_service.py`  
- `apps/mobile/src/services/api.ts`  
- `apps/mobile/src/services/speech-to-text.ts`  
- `apps/mobile/src/services/voice-orchestration.ts`  
- `apps/mobile/src/__tests__/services/speech-to-text.test.ts`  
- `.env.example`, `README.md`, `SETUP_GUIDE.md`, `START_SERVER.md`, `docs/azure-setup-guide.md`

---

## Dev Agent Record

**Agent Model Recommended:** Claude 3.5 Sonnet or GPT-4o mini (coding)  
**Complexity:** Medium – combines backend, mobile, and docs work.  
**Risks:** Handling large audio uploads on mobile web; ensuring Azure OpenAI preview API quotas.  
**Mitigations:** Use short audio fixtures in tests, expose timeout controls, retry on 429 with exponential backoff.

---

## QA Checklist (to fill during implementation)

- [ ] Uploaded WAV returns transcript with accuracy ≥ 95% in manual tests.  
- [ ] Error messages verified in Vietnamese for network loss, invalid audio, 401.  
- [ ] Expo app recovers from repeated voice submissions without leaking resources.  
- [ ] Backend logs include deployment name and request duration.  
- [ ] curl example in README tested and copy-paste ready.
